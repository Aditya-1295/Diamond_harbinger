# -*- coding: utf-8 -*-
"""diamond_trial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WJiSsQaTo5N9aiKiyEWqe2fofiO4VPP9
"""

import numpy as np
import pandas as pd
from datetime import datetime
import seaborn as sns

data=pd.read_csv('diamonds.csv')
data.shape

data.rename({"Unnamed: 0":"a"}, axis="columns", inplace=True)
data.drop(["a"], axis=1 , inplace=True)
data.info()

data.isnull().any

data.describe()

data=data.loc[(data[['x','y','z']]!=0).all(axis=1)]
data.describe()

data.loc[(data['x']==0) | (data['y']==0) | (data['x']==0) ]

data.corr()

data['cut']=data['cut'].map({'Ideal':1,'Good':2,'Very Good':3,'Fair':4,'Premium':5})
data['color']=data['color'].map({'E':1,'D':2,'F':3,'G':4,'H':5,'I':6,'J':7})
data['clarity']=data['clarity'].map({'VVS1':1,'IF':2,'VVS2':3,'VS1':4,'I1':5,'VS2':6,'SI1':7,'SI2':8})

np.isfinite(data).all()

data.dtypes

data['price'].unique()

from scipy import stats
stats.pearsonr(data['carat'],data['price'])
stats.pearsonr(data['color'],data['price'])

sns.heatmap(data.corr(),annot=True,fmt='.1g')

data['price/wt']=data['price']/data['carat']
print(data.groupby('cut')['price/wt'].mean().sort_values())
print(data.groupby('color')['price/wt'].mean().sort_values())
print(data.groupby('clarity')['price/wt'].mean().sort_values())
#data = data.drop(['price/wt','table'], axis=1)
data.corr()

data['cut/wt']=data['cut']/data['carat']
data['color/wt']=data['color']/data['carat']
data['clarity/wt']=data['clarity']/data['carat']
data['depth/wt']=data['depth']/data['carat']
data = data.drop(['cut','color','clarity','depth','price/wt','table'], axis=1)
data.corr()

import matplotlib.pyplot as plt
plt.scatter(data['depth/wt'],data['price'])
plt.show()
plt.scatter(data['cut/wt'],data['price'])
plt.show()

from sklearn.model_selection import train_test_split
x=data.drop(['price'],axis=1)
y=data['price']
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)

"""#LINEAR REGRESSION"""

from sklearn import linear_model
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error

from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree = 2)
x_train = poly.fit_transform(x_train)
x_test = poly.fit_transform(x_test)
reg_all = linear_model.LinearRegression()
reg_all.fit(x_train,y_train)
y_pred=reg_all.predict(x_test)
mae = mean_absolute_error(y_test,y_pred)
print("mae: %f" %(mae))
Rsquare=reg_all.score(x_test,y_test)
print("Rsquare: %f" %(Rsquare))
rmse=np.sqrt(mean_squared_error(y_test,y_pred))
print("rmse: %f" %(rmse))

from sklearn.tree import DecisionTreeRegressor
regressor = DecisionTreeRegressor(random_state = 0)
regressor.fit(x_train, y_train)
y_pred = regressor.predict(x_test)
mae = mean_absolute_error(y_test,y_pred)
print("mae: %f" %(mae))
Rsquare=regressor.score(x_test,y_test)
print("Rsquare: %f" %(Rsquare))
rmse=np.sqrt(mean_squared_error(y_test,y_pred))
print("rmse: %f" %(rmse))

import pickle as pk
pk.dump(regressor,open('model.pkl','wb'))

model=pk.load(open('model.pkl','rb'))